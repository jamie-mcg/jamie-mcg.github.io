<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Parallel Training the Transformer Architecture | Jamie McGowan </title> <meta name="author" content="Jamie McGowan"> <meta name="description" content="Schemes for parallelism of LLMs during training."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/artificial-intelligence.png?696be420326cea5b9a828337e7a77226"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jamie-mcg.github,io/assets/blogs_in_progress/2025-02-17_training-transformers/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jamie </span> McGowan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Parallel Training the Transformer Architecture</h1> <p class="post-meta"> January 29, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> LLM engineering     ·   <i class="fa-solid fa-tag fa-sm"></i> deep learning   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In order to be able to train the worlds largest machine learning models, many engineering techniques and strategies must be employed. But before we get onto those, lets just set the scene a little!</p> <p>When you train your first ever neural network using a GPU, chances are that you will be doing this on a single chip. This is great but at some point, this chip will be bottlenecked by its memory bandwidth and/or the number of FLOPs it can execute. What do we mean by this? Well… every GPU or TPU will have an on-chip memory wherby information is temporarily stored before being transferred to the computational elements in the processor, the rate at which this memory transfer can happen is determined by the <em>memory bandwidth</em> and the rate at which the processor can process the information is determined by the <em>FLOPs</em> (Floating Point Operations). So, in order to scale models efficiently, overcome fundamental constraints such as VMEM limits, and keep training to within a reasonable timescale, it becomes necessary to introduce multiple chips into a cluster.</p> <p>As you can imagine, this increase in compute gives us a few more options in how to arrange our training pipeline. But also some more potential bottlenecks to consider which are related to balancing the utilisation of all chips (how much computation they are being used for at any one time) with the inter-chip communication latency – in particular, we will see how we can attempt to overlap these two in order to ensure as high utilisation as possible during training.</p> <h2 id="an-mlp-perspective">An MLP Perspective</h2> <p>To begin, we will consider parallelisation of an MLP</p> <h3 id="data-parallelism">Data Parallelism</h3> <p>In this setting, we shard activations along the batch dimension.</p> <ul> <li>There is no communication in the forward pass</li> <li>The gradients are calculated layer by layer and then asynchronously AllReduced while the rest of the backpropagation happens.</li> <li>These AllReduce operations are not in the critical path</li> <li>This is pretty forgiving since the communication costs can be overlapped with the computation - as long as the comms cost &lt; compute cost.</li> <li>We can arbitrarily increase the batch size as long as we have more chips. In training, activations can dominate the memory footprint so this is very important.</li> <li>This does not reduce memory cost from the model or optimizer states.</li> <li>If the model &amp; states don’t fit on a single device, which is common, we cannot properly train with pure data parallelism.</li> <li>bf16 params and fp32 optimizer states (Adam has 2 moments), then we have 2 + 2 * 4 = 10 bytes per parameter.</li> </ul> <p>When are we communication bottlenecked?</p> <ul> <li>We have two AllReduces per MLP block, each of size 2 * d_hidden * d_in.</li> <li>We have the per-chip FLOPs and the bandwidth. We can calculate the compute time and communication time.</li> <li>No need to do this for the forward pass, since there is no communication.</li> <li>An AllReduce time depends on the total bytes and the bandwidth available.</li> <li>The total time is the maximum of either the comms or compute time.</li> <li>To remain compute bound, we need the per-device batch size to exceed the operational intensity.</li> <li>Computation time scales with the per-device batch size, but the communication time is independent of this because we are transferring the model weights.</li> </ul> <p>Context parallism:</p> <ul> <li>A batch is made up of sequences of tokens.</li> <li>We can do data parallelism across the batch and sequence dimension: which is called context parallelism,</li> <li>Attention is trickier because we do some cross-sequence computation.</li> <li>This can be handled by gathering KVs or Qs and overlapping compute and communication: ring attention.</li> </ul> <h3 id="fully-sharded-data-parallelism-fsdpzero-3">Fully-sharded Data Parallelism (FSDP/ZeRO-3)</h3> <p>The activations are still sharded along the batch dimension but also the rows of the parameters, gradients and optimizer states are sharded across the same axis.</p> <ul> <li>Model and optimizer states are sharded across data parallel shards</li> <li>FSDP drastically reduces per-device memory usage and saves backward pass FLOPs</li> <li>Couple of AllGathers in the forward pass to gather the sharded parameters before running the computation.</li> <li>These can be done while computing the previous layer activations.</li> <li>2 ReduceScatters to reduce gradients across data shards scatter the gradients across devices in backward pass.</li> <li>2 AllGathers in backward pass to gather the weight matrices to compute gradients across data sharded dimension.</li> <li>Also called ZeRO Sharding because we don’t perform any unnecessary compute or store any unnecessary state.</li> <li>The 1, 2, 3 refers to the sharding of optimizer states, gradients and weights.</li> <li>All these have the same communication cost so we can always do ZeRO-3 sharding.</li> <li>Data parallelism does a lot of duplicated work i.e. AllReduces the full gradient, updates the optimizer state/parameters.</li> <li>Instead, we can do ReduceScatter on the gradients and update only a shard of the optimizer state/parameters. Then AllGather parameters in the forward pass.</li> </ul> <p>When are we communication bottlenecked?</p> <ul> <li>The FLOPs and comms costs are exactly the same as the data parallelism case.</li> <li>However, the forward pass also has some communication in this case.</li> <li>But the communication bottleneck happens at the same point as in the data parallelism.</li> <li>So, as long as we satisfy the data parallelism bounds, we can upgrade to FSDP with no extra cost.</li> <li>Although we have added some communication cost to the forward pass, this overlaps with the compute.</li> <li>DeepSeek-v2 used a 40M batch size, which allows scaling to 47k chips or 5 TPUv5 pods before hitting a bandwidth limit.</li> </ul> <p>Critical batch size:</p> <ul> <li>We become comms bottlenecked if our batch size decreases.</li> <li>Data Parallelism and FSDP allow us to scale to arbitraily many chips, as long as we increase our batch size.</li> <li>However, larger batch sizes make training see diminishing returns due to noise-free gradients.</li> </ul> <h3 id="model-parallelism">Model Parallelism</h3> <p>Here, activations and parameters are sharded across the model dimension. In between operations, there are AllGather and ReduceScatter operations.</p> <ul> <li>In FSDP we move weights across chips. We can also shard the feedforward dimension and move thr activations during the layer.</li> <li>This unlocks a smaller efficient batch size per pod.</li> <li>The forward pass steps include AllGathering the activations, performing the computation, then reduce scattering the output activations</li> <li>The backwards pass includes an AllGather on the output and saved input, then performing a ReduceScatter on the final gradient at the input.</li> <li>$$</li> </ul> <h3 id="pipeline-parallelism">Pipeline Parallelism</h3> <p>The idea of pipeline parallelism is to shard the model across its layer dimension.</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jamie McGowan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>