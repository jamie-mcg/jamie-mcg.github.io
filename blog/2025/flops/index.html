<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MLPs Are All You Need... For FLOP calculations | Jamie McGowan </title> <meta name="author" content="Jamie McGowan"> <meta name="description" content="A run down of the major operations you need to consider when calculating the computational burden from your transformer"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/artificial-intelligence.png?696be420326cea5b9a828337e7a77226"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jamie-mcg.github,io/blog/2025/flops/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jamie </span> McGowan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MLPs Are All You Need... For FLOP calculations</h1> <p class="post-meta"> February 20, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>     ·   <a href="/blog/category/engineering"> <i class="fa-solid fa-tag fa-sm"></i> engineering</a>   <a href="/blog/category/llm"> <i class="fa-solid fa-tag fa-sm"></i> LLM</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>OK, clearly this section heading is misleading. But let me explain! So, transformer blocks are essentially made up of an attention part and a feedforward part. This feedforward block is further composed of three dense layers as below:</p> <p>In maths, for a model with \(d_{\text{model}}\) embedded/model dimension and \(d_{\text{hidden}}\) hidden dimension, and an input with batch size \(N\) and sequence length \(T\), this can be written as:</p> \[X_{\text{out,ff}} = \sigma(X_{\text{in}} W_2 \odot X_{\text{in}} W_1) W_3\] <p>where \(X_{\text{in}} \in \R^{N\times T \times d_{\text{model}}}\), \(W_{1,2} \in \R^{d_{\text{model}} \times d_{\text{hidden}}}\) and \(W_{3} \in \R^{d_{\text{hidden}} \times d_{\text{model}}}\).</p> <p>Now, lets work out the computational burden this function has for a forward pass!</p> <ul> <li>There are two operations that require \(2NTd_{\text{model}}d_{\text{hidden}}\) FLOPs and memory \(d_{\text{model}}d_{\text{hidden}}\)</li> <li>An element-wise multiplication which is of the order \(NTd_{\text{hidden}}\) and no memory overhead</li> <li>A final operation which requires a further \(2NTd_{\text{model}}d_{\text{hidden}}\) and memory \(d_{\text{model}}d_{\text{hidden}}\)</li> </ul> <p>So in total we have \(6NTd_{\text{model}}d_{\text{hidden}}\) FLOPs.</p> <p>By contrast, for the Attention part, we have the following formulation:</p> \[A^h = \text{softmax} \left(\frac{X_{\text{in}}W^h_q \cdot {W_k^{h}}^T X_{\text{in}}^T}{\sqrt{d_h}}\right) \\ Y^h = A^h \cdot X_{\text{in}}W^h_v \\ X_{\text{out}} = Y^{h} W_{o}\] <p>where \(d_{\text{h}}\) is the attention head dimension, \(W_{q,k,v}^h \in \R^{d_{\text{model}} \times d_{\text{h}}}\), \(h\) is the attention head index which runs up to the total number of attention heads \(H\) and \(W_{o} \in \R^{H d_h \times d_{\text{model}}}\) .</p> <ul> <li>There are three multiplications in here, each requiring \(2NTd_{\text{model}}d_{\text{h}}H\) and \(Hd_{\text{model}}d_{\text{h}}\) memory.</li> <li>There is a dot product in the softmax operation which requires \(2NT^2d_{h}H\) FLOPs (where we have a squared sequence length because we’re creating the lookup table over all tokens in the sequence).</li> <li>Similarly, we have another dot product between the attention matrix and the value which involves \(2NT^2d_{h}H\).</li> <li>Finally, we have the last matmul operation which multiplies and reduces all the attention head outputs into our final output, which requires \(2NTd_{\text{model}}d_{\text{h}}H\) and \(Hd_{\text{model}}d_{\text{h}}\) memory.</li> </ul> <p>In total, that leaves us with \(8NTd_{\text{model}}d_{\text{h}}H + 4NT^2d_{h}H\) FLOPs, where the first term is from the MLP block and the second is all down to attention.</p> <p>Just to analyse this a bit more, lets investigate the relative difference between these two terms. Factoring common terms out we get:</p> \[4NTd_{h}H(2d_{\text{model}} + T) \simeq 8NTd_{\text{model}}d_{\text{h}}H \qquad \text{when} \quad d_{\text{model}} \gg T/2\] <p>where the MLPs dominate the FLOP count whenever \(d_{\text{model}}\) is higher than the context size.</p> <p>Right, so now lets just put this into context. Let’s consider LLaMA 3-70B, which has \(d_{model} = 8192\) and therefore we can get a pretty good approximation to the compute costs of this model whenever our sequence length is less than ~4k tokens!</p> <h3 id="a-quick-note-on-training">A quick note on training</h3> <p>The above was all done for inference, of course, for training things get slightly more complicated in terms of memory and FLOPs - especially when you start considering different checkpointing strategies for intermediate activations, different optimizer states, and other parallelisation techniques. However, we can make a simple adaptation to the FLOP calculations we did above but just considering the chain rule and backpropagation.</p> <p>So, in training, gradients are essential for us to compute. Now imagine a set of feedforward layers stacked on top of each other, the computational graph produced from a 2-layer network looks like Figure 2.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/FLOPs/backprop-graph-480.webp 480w,/assets/img/blogs/FLOPs/backprop-graph-800.webp 800w,/assets/img/blogs/FLOPs/backprop-graph-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blogs/FLOPs/backprop-graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Computational graph from torchviz of a 2-layer MLP with annotations to show the leaf node computations (red) and backpropgated vectors for use in previous layers through vector-Jacobian products (green). Image created by author. </div> <p>The goal of backpropagation is to compute a gradient with respect to the current layer weights (this is what’s called a “leaf” in your computational graph), as well as the gradient with respect to the input of that layer.</p> <p>Why do we need these? Well, the former we’re going to use in our update equation to update the weights of that layer (red path in Figure 3), and the latter we are going to pass down to the previous layer as our new vector in the vector-Jacobian-product chain (green path in Figure 3).</p> <p>Don’t worry too much about the details of this here! All you need to really know is that there are 2 extra computations in the backwards pass, so you basically just want to add a factor of 3 to all the FLOP calculations we did above!</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jamie McGowan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>