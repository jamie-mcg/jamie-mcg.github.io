<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="introduction">Introduction</h2> <h2 id="setting-up-our-playground">Setting up our playground</h2> <p>Althogh simple, linear regression is a powerful setting for us to gain intuition around more complex neural network phenomena. Indeed, with this extremely simple playground, many questions surrounding the dynamics of optimization can be answered analytically.</p> <p>Let’s begin with a simple linear regression model that has a mapping $R^{N}\rightarrow R$ (i.e. the output is a scalar value):</p> \[y = \mathbf{w}^{T}\mathbf{\psi}(\mathbf{x}) + b\] <p>where $\mathbf{\psi}(\mathbf{x}) \in R^{N}$ is the feature representation of the input vector $\mathbf{x} \in R^{N}$ and $\mathbf{w} \in R^{N}$ is a weight vector that, together with the bias $b \in R$, defines the parameters of our toy model.</p> <p>Next, every great model needs a problem to solve. So let’s take our loss function to be a regression loss of:</p> \[\mathcal{L} = \frac{1}{2}||\mathbf{w}^{T}\mathbf{\psi}(\mathbf{x}) + b - \hat{y}||^{2}\] <p>where $\hat{y} \in R$ is the true value we are attempting to predict for some input $\mathbf{x}$.</p> <p>Given some data pairs $\mathcal{D} \sim {\mathbf{x}<em>{i}, y</em>{i}}^{N}_{i=1}$, we now wish to minimise the function $\mathcal{L}$ above with</p> <p>\(w^{*} \in argmin_{w} \mathcal{L}(\mathbf{w})\) \(= argmin_{w} \frac{1}{2N} \sum_{i=1}^{N} (\mathbf{w}^{T}\mathbf{\psi(\mathbf{x}_{i})} + b - \hat{y}_i)^{2}\) \(= argmin_{w} \frac{1}{2N} ||\mathbf{W}^{T}\mathbf{\Psi(\mathbf{x}_{i})} - \hat{Y}||^{2}\)</p> <p>where in the above, we have absorbed the bias into the weight vector $\mathbf{W} = (\mathbf{w}, b)$ and defined $\mathbf{\Psi} = (\mathbf{\psi}, 1)$.</p> <p>Note that in the 1st line of the argmin equation above, we also use the symbol $\in$ as the minimum is by no means guaranteed to be unique - in fact it is much more likely in general to be part of a set. In these cases, the optimum that we find during training will depend entirely upon the dynamics.</p> <p>Let’s take a quick look at what we have so far though. In particular, the function we really want to understand is the loss function!</p> <p>Note that in the above we can see this loss function is indeed a convex quadratic, meaning that our dynamics should be very well behaved (i.e. positive curvature everywhere and a unique global minimum).</p> <p>Aside: Everyone always throws the term convex about in the literature as a way to explain away a lot of training dynamics. When you’re starting out, it is easy to overlook what this term really means! Take a look at my other post on convexity to give yourself a brief primer on this subject!</p> </body></html>