<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jamie-mcg.github,io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jamie-mcg.github,io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-29T15:11:11+00:00</updated><id>https://jamie-mcg.github,io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Convexity Explained</title><link href="https://jamie-mcg.github,io/blog/2024/convexity/" rel="alternate" type="text/html" title="Convexity Explained"/><published>2024-02-11T15:12:00+00:00</published><updated>2024-02-11T15:12:00+00:00</updated><id>https://jamie-mcg.github,io/blog/2024/convexity</id><content type="html" xml:base="https://jamie-mcg.github,io/blog/2024/convexity/"><![CDATA[<h2 id="what-is-convexity">What is convexity?</h2> <blockquote> <p>A function is convex if the line segment between two points lies above the function.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/convexity/jensens-480.webp 480w,/assets/img/blogs/convexity/jensens-800.webp 800w,/assets/img/blogs/convexity/jensens-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/convexity/jensens.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A visual representation of a convex function with an intersection forming an enclosed area above the curve. Figure created by author. </div> <p>In the above figure, we see an illustrative example of the definition above. Here, we can see that if the intersection of the line forms an enclosed area which is uninterrupted above the curve, the function can be considered convex.</p> <p>We can also see that if the opposite is true i.e. the enclosed area is below the curve, the function is concave!</p> <p>Indeed, a convex function \(f(x)\) can be reflected into a concave function \(-f(x)\).</p> <p>Mathematically, convexity is best described by Jensen’s inequality,</p> <p>\(f(E[X]) \le E[f(X)]\) for a convex function \(f\) and a random vector \(X\) that is within the domain of \(f\). Now, I find it hard to conceptualise inequalities at face value, so let’s chat about it a bit…</p> <p>Jensen’s inequality essentially states that the expected value of a convex transformation of a random variable (i.e. \(E[f(X)]\)) is more than or equal to the value of the convex function evaluated at the mean od the random variable (i.e. \(f(E[X])\)) - or the gap between these two is never negative.</p> <p>OK, I’ll be honest, when I wrote that, it only half made sense to me… I’m a physicist by background and so I love drawings and building intuitions visually, so let’s do that!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/convexity/jensens_inequality-480.webp 480w,/assets/img/blogs/convexity/jensens_inequality-800.webp 800w,/assets/img/blogs/convexity/jensens_inequality-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/convexity/jensens_inequality.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An animated representation of Jensen's inequality. Figure created by author. </div> <p>Consider the figure above, here it is a lot easier to see what Jensen’s inequality is really saying. Basically the LHS of Jensen’s inequality is represented by the green dotted line - this is taking the average of the two red \(x\) points and then evaluating the function at this point (i.e. \(f(E[X])\)). Whereas, the intersection of the red dashed line and the green dotted line is what the RHS of Jensen’s inequality is describing - evaluating the function before taking the expectation (i.e. \(E[f(X)]\)).</p> <p>We can see that over the entire range of this function, the green dot never crosses the red line and so the intersection of the green dotted and red dashed lines is always above the blue solid curve.</p> <p>OK, hopefully that was a nice and easy to follow explanation for you! And if it wasn’t, at least you got to distract yourself with a cool animation! Anyway, let’s get back to convexity…</p> <p>OK, so as well as Jensen’s inequality, for a twice differentiable function which is convex, we have two rather nice conditions:</p> \[f^{''}(x) &gt;= 0 \qquad \forall x, \\ f(y) &gt;= f(x) + \nabla f(x)^{T} (y-x).\] <p>The first of these conditions tells us that curvature must be positive everywhere. The second condition ensures that the line between two points \(x\) and \(y\) lies above the function within that interval (illustrated earlier).</p> <p>Following on from this, the first condition ensures that a convex function must curve upwards (or not at all). Therefore, if we find a minima x, any movement away from this point will result in an increase in the function value.</p> <p>Together, these conditions are enough to guarantee that any minima found in a convex function must be a global minima (although this doesn’t necessarily need to be unique). This is extremely useful for techniques such as stochastic Gradient Descent since if we find a local minima (something which Gradient Descent generally tends to find), we can be certain that this in fact the global minima - the best possible solution.</p> <h2 id="the-strong-the-strict-and-the-standard">The Strong, the Strict and the Standard</h2> <p>As always with math, we don’t just have one type of something, we have some vague terms that sound cool to describe some more features.</p> <p>Convex functions are no exception and can be further categorised with three properties:</p> <ul> <li>Convex</li> <li>Strictly Convex</li> <li>Strongly Convex</li> </ul> <p>Now, the further you move down that list, the stronger these properties become (hence the “strongly” term). This just means that the subset of convex functions that have these properties becomes smaller due to stronger constraints.</p> <p>So far in this post, we have been talking about convexity in its most general form, where \(f^{''}(x) &gt;= 0\). This is the condition for ‘convex’ functions.</p> <p>Strictly convex functions are those which satisfy \(f^{''}(x) &gt; 0\), i.e. the curvature can never be 0.</p> <p>Stongly convex functions are those which satisfy \(f^{''}(x) &gt;= m &gt; 0\), where the curvature is non-vanishing and stays bounded below by some positive value \(m\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/convexity/strong_and_strict-480.webp 480w,/assets/img/blogs/convexity/strong_and_strict-800.webp 800w,/assets/img/blogs/convexity/strong_and_strict-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/convexity/strong_and_strict.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A plot showing examples of different types of convex functions. Figure created by author. </div> <p>In the above plot, we can see the differences between these types of convexities visually. If you take the definitions in the legend and differentiate these twice then it is easy to prove to yourself that each of these are convex, strictly convex or strongly convex.</p> <p>OK cool, well done for reading another one of my posts! I’m trying to keep these as short and sweet as possible and just give the “need-to-know” understanding for these concepts. Let me know how I did!</p>]]></content><author><name></name></author><category term="mathematics"/><category term="math"/><category term="optimisation"/><summary type="html"><![CDATA[A short and easy to follow primer on convexity.]]></summary></entry><entry><title type="html">What the Lipschitz?!</title><link href="https://jamie-mcg.github,io/blog/2024/lipschitz/" rel="alternate" type="text/html" title="What the Lipschitz?!"/><published>2024-01-29T15:12:00+00:00</published><updated>2024-01-29T15:12:00+00:00</updated><id>https://jamie-mcg.github,io/blog/2024/lipschitz</id><content type="html" xml:base="https://jamie-mcg.github,io/blog/2024/lipschitz/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Lipschitzness of a function is essential for ensuring the convergence properties of many gradient descent algorithms</p> <h2 id="definition">Definition</h2> <p>Simply put, Lipschitz functions are those which do not explode for some value \(x\). So, functions which change too fast and/or become infinitely steep are not Lipschitz functions.</p> <p>More formally, let \(\chi \in \mathbb{R}^{d}\) be a d-dimensional subspace of real values. If we take a function \(f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{n}\) which provides a mapping from a d-dimensional space to a p-dimensional space, we can say that \(f\) is \(L\)-Lipschitz over \(\chi\) if and only if we have:</p> \[|f(x_{2}) - f(x_{1})| \le L | x_{1} - x_{2} | \qquad \forall x_{1}, x_{2} \in \chi\] <p>I always think this looks a bit confusing written this way, so a more intuitive way to write it is simply:</p> \[\frac{|f(x_{2}) - f(x_{1})|}{ | x_{1} - x_{2} | } \le L \qquad \forall x_{1}, x_{2} \in \chi\] <p>where we now have the form of \(\Delta y / \Delta x\) on the left hand side.</p> <p>Thinking about this a bit more, this condition demands that the slope of the secant line between two points \(x_{1}\) and \(x_{2}\) must be between \(-L \le m \le L\).</p> <h3 id="example-is-cosine-lipschitz">Example: Is cosine Lipschitz?</h3> <p>Start by employing the definition above,</p> \[\frac{f(x) - f(y)}{x - y} \simeq f^{'}(x) = \text{sin}(x)\] <table> <tbody> <tr> <td>We know that $$</td> <td>\text{sin}(x)</td> <td>\le 1$$, so we can rewrite this as:</td> </tr> </tbody> </table> \[|f(x) - f(y)| \le L | x - y |\] <p>where \(L = 1\). So we say that cosine is a \(1\)-Lipschitz function.</p> <p>Now, if you’re screaming “Stop showing me maths!!”, you’re in luck, because I’ve created some nice plots for us to look at…</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/lipschitz/lipschitz_curves-480.webp 480w,/assets/img/blogs/lipschitz/lipschitz_curves-800.webp 800w,/assets/img/blogs/lipschitz/lipschitz_curves-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/lipschitz/lipschitz_curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Plots showing linear, cosine, cusp and quadratic functions to showcase Lipschitzness. Image created by author. </div> <p>So, remembering our rule from above, we can see that the linear and cosine functions both satisfy the Lipschitz inquality as there is some bound on the slopes they exhibit for all \(x \in \chi\). On the other hand, the cusp function is not Lipschitz continuous at the origin because it has a discontinuity. Finally, the quadratic function is defined as Lipschitz continuous if and only if we are considering a bounded interval, because as \(x \rightarrow \infty\), the slope becomes arbitrarily large.</p> <h2 id="globally--locally-lipschitz">Globally &amp; Locally Lipschitz</h2> <p>Just to round off this small post, I want to talk a bit about global and local Lipschitz functions. The above section kind of describes functions which are globally Lipschitz since we haven’t defined a subset of the function space to consider, so here, we’ll start local!</p> <p>Say for example, we have some function \(f\) which is locally Lipschitz for a compact subset of \(\chi\). We’ll call this \(\Omega \subset \chi \in \mathbb{R}^{d}\).</p> <p>For the local Lipschitz property to hold, it must be true that there is a constant \(L_{\Omega}\) such that,</p> \[|f(x) - f(y)| \le L_{\Omega} | x - y | \qquad \forall x_{1}, x_{2} \in \Omega\] <p>where \(L_{\Omega}\) can indeed depend on the subset \(\Omega\). For example, the function \(f(x) = x^{2}\) has an \(L_{\Omega}\) which depends on \(x\) (since only one of the \(x^{2}\) will cancel). Therefore as the subset \(\Omega\) becomes larger, \(L_{\Omega}\) will scale linearly with this.</p> <p>The above example defines a situation where we have local Lipschitzness but not global!</p> <p>For global Lipschitzness, we require the function to have a Lipschitz constant which does not depend on the subset \(\Omega\), i.e. \(L_{\Omega} = L\).</p> <p>Now, just looking back at our plot from earlier, we can observe that the linear and cosine functions are most definitely locally Lipschitz because they are globally Lipschitz continuous. But although the cusp and quadratic functions were not globally Lipschitz, they can indeed be locally Lipschitz. For example, the problem with the cusp function was just the point at the origin. So if we just ignore this point (define our compact subspace accordingly), we can find a Lipschitz constant. Then, for our quadratic function, we said that it needed to be a bounded interval before we could call it Lipschitz continuous. But the definition of local Lipschitzness is that the “local” is on a compact subspace which, you guessed it, is bounded! Thus a Lipschitz constant exists for any compact subset on the quadratics domain.</p> <p>I hope this post has been a useful primer into the property of Lipschitzness. As always, feel free to reach out with any comments/questions!</p>]]></content><author><name></name></author><category term="mathematics"/><category term="math"/><summary type="html"><![CDATA[All the math and intuition you need to know for a happy life.]]></summary></entry><entry><title type="html">A Technical Introduction to Diffusion Models</title><link href="https://jamie-mcg.github,io/blog/2024/diffusion/" rel="alternate" type="text/html" title="A Technical Introduction to Diffusion Models"/><published>2024-01-29T15:12:00+00:00</published><updated>2024-01-29T15:12:00+00:00</updated><id>https://jamie-mcg.github,io/blog/2024/diffusion</id><content type="html" xml:base="https://jamie-mcg.github,io/blog/2024/diffusion/"><![CDATA[<p>Generative Models are a class of networks which have left many of us spell-bounded in recent years. These models have propelled Artificial Intelligence (AI) into popular culture and captivated the general public by their attempts to encapsulate something inately human; <em>creativity</em>.</p> <p>In parallel to many intricate “under-the-hood” technical advancements, the humanisation of AI has played a major role in driving the popularity of generative models. The ease of use and access to some of the most powerful AI tools the world has seen, has motivated a widespread adoption of generative models into many services and third-party applications.</p> <p>Indeed, as Uncle Ben would rightly say:</p> <p><em>with great power, comes great responsibility</em></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion/giphy-480.webp 480w,/assets/img/blogs/diffusion/giphy-800.webp 800w,/assets/img/blogs/diffusion/giphy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion/giphy.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Spider-man classic scene, image available <a href="https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExejdpdHBobW94Y2RmYWw1NHNjc3VjZzk0OTR3MDB2Ymg1OWNkc3o5ayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/10KIsXhwdoerHW/giphy.gif">here</a>. </div> <p>and AI is no exception, with many of these new possibilities bringing with them contraversies and a new requirement for AI governance.</p> <p>Anyway, we can cover AI safety and governance in another blog post. For now, in this post, I want to explain how diffusion models really work and discuss some of their incredible applications.</p> <h2 id="problem-setup">Problem Setup</h2> <p>As with all Machine Learning problems, we must start with the data! The goal of generative modelling solutions is to find some model for the data distribution such that generated outputs lie within the distribution of inputs.</p> <p>Let’s just contextualise that with an example: say I asked you to imagine a car. It is highly likely that the image you choose to imagine would be similar to those you have seen in the past. In this case, your brain has a model of what a car should look like, that has been learnt from all your experience of seeing cars in the past. Therefore we can say that, the imagined output has a high probability of being close to one which is realistic to find in the wild.</p> <p>To be clear, this doesn’t mean that we can imagine exactly a particular image of a car and reproduce it in our head (unless you are a slightly obsessive petrolhead…), it means we recognise the key concepts that are required for a car to be, well… a car, and we are able to compose these concepts together into something that resembles a car. Indeed, our model is not discrete and is also able to interpolate between many concepts to produce something entirely different to what we may have seen in the past. Without sounding too philisophical, this is arguably one of the pillars of human-like creativity, the compositionality of learned concepts.</p> <p>Now let’s just round off this section by stating some mathematical definitions which will be useful later.</p> <p>Let \(x \in RR^d\) be our data with dimension $d$ which is obtained from some underlying distribution $q_{\text{data}}(x)$. The goal of a generative model is to estimate $p(x)$ which approximates the true data distribution $q_{\text{data}}(x)$.</p> <p>Here is an example of a model fabricating some image out of thin air!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion/diffusion_process-480.webp 480w,/assets/img/blogs/diffusion/diffusion_process-800.webp 800w,/assets/img/blogs/diffusion/diffusion_process-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion/diffusion_process.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure created by author with astronaut image from `skimage`. </div> <p>So, how do we do this?</p> <h2 id="setting-up-the-training-algorithm">Setting up the training algorithm</h2> <p>We have data, but we don’t have any labels, so at the moment we’re in an unsupervised regime. Typically learning from unsupervised data tends to be quite hard and is also somewhat hard to control - since you are leaving the model up to its own devices.</p> <p>A much easier problem would be a supervised task, so let’s try and set one of these up from the ingredients we have.</p> <p>In much the same way as you would start from a blank page when asked to draw a picture, we can ask our probabilistic model to do the same. In this setting, we have the input as a randomised vector of the same dimension of $x$ and $x$ becomes our target! Voila!</p> <p>Right, before we get confused, lets add to our math we had earlier, just so we can digest this.</p> <p>Let our data be $x^{(0)}$ and our starting point (blank page/random vector) be $x^{(T)}$. Apologies for the cryptic notation, I promise this notation will become clearer later…</p> <table> <tbody> <tr> <td>The problem we have set up is $x^{(0)} \sim p(x^{(0)}</td> <td>x^{(T)})$ as an approximation to $x \sim q(x)$.</td> </tr> </tbody> </table> <p>Hmm… this still seems super hard though, right? If I asked you to go from a blank page to a masterpiece in a single step, unless you were Picasso, you would quite clearly struggle. So let’s break it up, step-by-step.</p> <p>Instead of going from $x^{(T)} \rightarrow x^{(0)}$ in 1 step, lets say it takes $T$ steps:</p> \[x^{(0)} \leftarrow x^{(1)} \leftarrow \dots \leftarrow x^{(t-1)} \leftarrow x^{(t)} \leftarrow \dots \leftarrow x^{(T-1)} \leftarrow x^{(T)}\] <table> <tbody> <tr> <td>where now we can define a much simpler regression problem at each step, which can be modelled by $x^{(t-1)} \sim p(x^{(t-1)}</td> <td>x^{(t)})$. With this, we can write the marginal distribution as:</td> </tr> </tbody> </table> \[p(x^{(0)}) = \int p(x^{(T)}) p(x^{(0)}|x^{(1)}) \dots p(x^{(T-1)}|x^{(T)}) dx^{(1)} \dots dx^{(0)} \\ = \int p(x^{(T)}) \prod_{t=1}^{T} p(x^{(t-1)}|x^{(t)}) dx^{(t)} \\ = \int p(x^{(0:T)}) dx^{(1:T)}\] <p>where the last line is a commonly used shorthand for the concatenation of variables across timesteps.</p> <p>OK, so the math checks out that as long as we obey the time ordering i.e. $x^{(T)}$ being the worst, $x^{(0)}$ being the best and $t=T-1,\dots 1$ being progressively better versions of $x$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion/diffusion_process-480.webp 480w,/assets/img/blogs/diffusion/diffusion_process-800.webp 800w,/assets/img/blogs/diffusion/diffusion_process-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion/diffusion_process.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure created by author with astronaut image from `skimage`. </div> <h2 id="creating-xt">Creating $x^{(t)}$</h2> <p>The first problem we have is that $x^{(0)} \sim q(x^{(0)})$ is a perfect data sample, so how do we go about getting our hands on $x^{(t)} \forall t \in {1, \dots, T}$.</p> <p>This is where “noising” comes in!</p> <p>The data creation process for our litter regression problems will involve noising our perfect data $x^{(0)}$ by a small amount over $T$ steps, until we reach $x^{(T)}$.</p> <table> <tbody> <tr> <td>We will denote the noising process as $q(x^{(t)}</td> <td>x^{(t-1)})$ and define it as a first order Gaussian which performs the transformation:</td> </tr> </tbody> </table> \[x^{(t)} = \lambda_t x^{(t-1)} + \sigma_t \epsilon_t \qquad \epsilon \sim \mathcal{N}(0,1) \\ q(x^{(t)}|x^{(t-1)}) = \mathcal{N}(x^{(t)}; \lambda_t x^{(t-1)}, \sigma_t^2)\] <p>Great! We now have our noising and denoising processing written down!</p> <p>Let’s see an example of how these probabilities are related. Below is an example of how one can start with some data distribution $q(x^{(0)})$ (with two distinct distributions) and end up with some noised Gaussian distibution - where all evidence of the original data has gone.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion/diffusion_process_probability-480.webp 480w,/assets/img/blogs/diffusion/diffusion_process_probability-800.webp 800w,/assets/img/blogs/diffusion/diffusion_process_probability-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion/diffusion_process_probability.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure created by author. </div> <h2 id="how-do-we-noise-this-thing">How do we noise this thing?</h2> <p>In order to actually use this noising process, we need to first define $\lambda_t$ and $\sigma_t$. If we stop and think about what these parameters actually mean for a second, we might be able to gain some insight.</p> <ul> <li>$\lambda_t \rightarrow 0$ means that we forget all information and only have noise</li> <li>$\lambda_t = 1$ means we retain the information in $x^{(t-1)}$</li> </ul> <p>We want $x^{(T)}$ to contain zero information, since $x^{(T)}$ is our blank canvas which we want to easily sample from whatever we want to generate something new.</p> <p>In this sense, we would like $\lambda_t &lt; 1$, but we want to keep small regression tasks as simple as possible - not lose too much information at each step. So we might want to have $\lambda_t$ somewhere close to 1 in practice.</p> <p>As long as $\lambda_t &lt; 1$, for $T \rightarrow \infty$, we are guaranteed to get $x^{(T)}$ as pure noise! - a perfect blank canvas for an ML model.</p> <p>Similarly for $\sigma_t$, this controls the amount of noise that is added at each step. Something we might want to put $\sim 0$ for the same reasons as above.</p> <p>A common choice is to set $\sigma_t^2 = 1 - \lambda_t^2$, which for now we will motivate by saying that when $\lambda_t \ge 1$, $\sigma_t^2 \ge 0$. But it is actually theoretically justified by guaranteeing that $\mathbb{E}<em>{q(x^{(t)})}[x^{(t)}] = 0$ and $\mathbb{E}</em>{q(x^{(t)})}[(x^{(t)})^2] = 1$.</p> <h3 id="variance-preserving-process">Variance Preserving Process</h3> <p>Aside: $\sigma_t^2 = 1 - \lambda^2<em>t$ ensures that $\mathbb{E}</em>{q(x^{(t)})}[x^{(t)}] = 0$ and $\mathbb{E}_{q(x^{(t)})}[(x^{(t)})^2] = 1$.</p> <p>We know that the data satisfies:</p> \[\mathbb{E}_{q(x^{(0)})}[x^{(0)}] = 0 \quad \text{and} \quad \mathbb{E}_{q(x^{(0)})}[(x^{(0)})^2] = 1\] <p>because it can be normalised to do so. So let’s consider $x^{(1)}$:</p> \[\mathbb{E}_{q(x^{(1)})}[x^{(1)}] = \mathbb{E}[\lambda_1 x^{(0)} + \sigma_1 \epsilon_1] = \lambda_1 \mathbb{E}[x^{(0)}] + \sigma_1 \mathbb{E}[\epsilon_1] = 0\] <p>because $\epsilon_1 \sim \mathcal{N}(0, 1)$. And also:</p> \[\mathbb{E}_{q(x^{(1)})}[(x^{(1)})^2] = \mathbb{E}[\lambda_1^2 (x^{(0)})^2 + \sigma_1^2 \epsilon_1^2] = \lambda_1^2 \mathbb{E}[(x^{(0)})^2] + \sigma_1^2 \mathbb{E}[\epsilon_1^2] = \lambda_1^2 + \sigma_1^2 \\ \therefore \mathbb{E}_{q(x^{(1)})}[(x^{(1)})^2] = 1 \iff \sigma_1^2 = 1-\lambda_1^2\] <p>By recursion, we can see that this is true for all $t$.</p> <p>In this case, when the process is variance preserving, we can write down:</p> \[q(x^{(t)}|x^{(0)}) = \mathcal{N}(x^{(t)}; \prod_{i=1}^{t} \lambda_{i} x^{(0)}, 1 - \prod_{i=1}^{t}\lambda_t^2)\] <table> <tbody> <tr> <td>which can be proved by unrolling $q(x^{(t)}</td> <td>x^{(t-1)})$, if you fancy doing some rather laborious maths.</td> </tr> </tbody> </table> <p>OK, now we are getting somewhere. We have a problem setup, we have our chain of makeshift supervised regression problems and we know how to generate the noisey labels for each of these regression problems. Indeed, from the above, we can even generate any noisey sample at any point in this chain, directly from the input $x^{(0)}$, as long as we have a variance preserving process!</p> <p>Practically, this means that we do not need to generate <em>all</em> the previous $[x^{(1)}, \dots x^{(t-1)}]$ just to get $x^{(t)}$, which will become important later when we talk about training!</p> <p>Just one thing to note here before we move on to the next section is that; in order for us to generate $x^{(0)}$ from $x^{(T)}$, which is essentially the aim of a diffusion model, we still require $x^{(T)}$ to be an <em>easy</em> sample to generate. To put this another way, when we deploy this model we will not have access to $x^{(0)}$, we need to generate it! So our starting point $x^{(T)}$ should ideally not have any information about $x^{(0)}$ left inside it, it should be complete noise - which is super easy to generate independent of any other sample in the chain.</p> <p>The above requirement basically tells us that we’re going to run into a balancing act between how <em>hard</em> we make each regression problem (how much noise we add per-step) and how many timesteps can afford to do. Anyway, more on this later!</p> <h2 id="defining-a-loss">Defining a Loss</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion/generated_samples-480.webp 480w,/assets/img/blogs/diffusion/generated_samples-800.webp 800w,/assets/img/blogs/diffusion/generated_samples-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion/generated_samples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure created by author. </div> <p>Taking a step back, we have a dataset that now consists of lots of ordered noisey samples which need to be assigined to a particular regression step in our denoising schedule (i.e., going from $T \rightarrow$ 0).</p> <p>To play aroung a bit, lets simply define a loss which concatenates all of our small regression tasks, each with their own set of parameters $\Theta = {\theta_t}_{t\in [0, T-1]}$.</p> \[\mathcal{L}(\Theta) = \mathbb{E}_q \log\left\{p(x^{(T)}) \prod_{t=1}^{T} p_{\theta_{t-1}}(x^{(t-1)}|x^{(t)}) \right\} \\ =\mathbb{E}_q \left[\log p(x^{(T)})\right] + \sum_{t=1}^{T} \mathbb{E}_q \left[\log p_{\theta_{t-1}}(x^{(t-1)}|x^{(t)})\right]\] <table> <tbody> <tr> <td>where it is the second term which depends on the parameters, so an individual regression loss would be $\mathcal{L}(\theta_{t-1}) = \mathbb{E}<em>q \left[\log p</em>{\theta_{t-1}}(x^{(t-1)}</td> <td>x^{(t)})\right]$.</td> </tr> </tbody> </table> <p>OK cool, so as we are all excellent researchers and like to think about the implications of our maths before whacking it into some code… Let’s just have a think about the complexity of the above formalism.</p> <p>Let $\theta_t \in \mathbb{R}^d$, then we will end up with a total parameter count of $T \times d$. OK, so this means that as the number of timesteps increases, this model is going to scale linearly and as $T \rightarrow \infty$, become intractable.</p> <p>OK, you’re right, we’ll never be actually taking $T \rightarrow \infty$ in practice. But we do need it to be pretty large to satisfy the requirements we discussed earlier, so its still a problem. One way we can try to solve this problem is <em>weight sharing</em>.</p> <p>From now on, we will define one set of weights for all timesteps $\theta_t \rightarrow \theta$, which reduces the total parameters to just $d$. This is not only motivated from a computational perspective, but also by the fact that if we have defined our chain of regression tasks correctly, they are all kind of doing the same thing. So intuitively, there should be some redundancy across timesteps.</p> <p>After all this, we can make a minimal adjustment to our individual loss function:</p> \[\mathcal{L}_{t-1}(\theta) = \mathbb{E}_q \left[\log p_{\theta}(x^{(t-1)}|x^{(t)})\right].\] <p>Right, last thing for this section, how do we add some control to this training? Let me explain…</p> <p>Simply summing all the individual losses for each step in our chain is fine, but it weights each step equally in terms of its feedback to $\theta$ - which are now shared across timesteps. However, you can imagine that there might be some steps that need to be <em>more accurate</em> than others in our chain.</p> <p>In order to add some contol, we will introduce some weighting parameters $w_t$ into our total loss which control how much each step contributes to updating $\theta$:</p> \[\theta^{*} = \text{argmin}_{\theta} \sum_{t=1}^{T} w_{t-1} \mathcal{L}_{t-1}(\theta)\] <h2 id="the-magic-of-stochastic-optimization">The Magic of Stochastic Optimization</h2> <p>Right now, you’re probably thinking, how much more can there be to this! But honestly, this is one of my favourite tricks that is used all over Machine Learning. Its super simple but when I first started learning about diffusion models, understanding how they leverage stochastic optimization made me think about the time axis (and also the sequence axis in transformers) in a much clearer way.</p> <p>Anyway, the above update rule has a sum over $T$ terms, all of which require varying numbers of passes through the network. This is fine but because we are sharing weights across timesteps, we can do better.</p> <p>To gain an intuition, lets go back to thinking about batches of data and training a very simple neural network with parameters $\theta$. Imagine our dataset $\mathcal{D}$ contains $N_\mathcal{D}$ samples. Theoretically, this gives us a solution of the form:</p> \[\theta^{*} = \text{argmin}_{\theta} \frac{1}{N} \sum_{n=1}^{N} w_{n} \mathcal{L}_{n}(\theta)\] <p>where $w_n$ are weighting scalars for how much each sample contributes - this could be related to the quality of that datapoint for example.</p> <p>This looks very similar to the equation we just wrote down for diffusion models, right? But in training a neural network we don’t need see <em>all</em> the $N_\mathcal{D}$ samples, we usually get away with defining a some batches and approximating the weighted sum via an expectation over the data:</p> \[\frac{1}{N} \sum_{n=1}^{N} w_{n} \mathcal{L}_{n}(\theta) = \mathbb{E}_{n \sim \mathcal{U}(1,N)}[w_n \mathcal{L}_{n}(\theta)]\] <p>This trick is possible because the parameters are shared across mini-batches. But remember, in our diffusion setup, we are also sharing parameters across $T$ timesteps, so we can employ exactly the same trick!</p> \[\theta^{*} = \text{argmin}_{\theta} \sum_{t=1}^{T} w_{t-1} \mathcal{L}_{t-1}(\theta) = \text{argmin}_{\theta} \left\{\mathbb{E}_{t \sim \mathcal{U}(1, T)} \left[w_{t-1} \mathcal{L}_{t-1}(\theta)\right]\right\}\] <p>Hooray, now we don’t need ALL $T$ terms! We can just sample timesteps randomly from a uniform distribution. But remember, because we have a variance preserving process, we can get the target for the sampled timestep directly from the data $x^{(0)}$.</p> <h2 id="the-denoising-step">The Denoising Step</h2> <p>Now we come to the denoising part of our pipeline… Finally!</p> <p>In this part, the goal is to learn the noise which we want to remove at each step. However, because we have already defined the noising schedule with a Gaussian we can also parameterise the noise to remove similarly:</p> \[p(x^{(t-1)}|x^{(t)}; t-1, \theta) = \mathcal{N}(x^{(t-1)};\mu_{\theta}(x^{(t)}; t-1), \sigma_{\theta}^{2}(x^{(t)}; t-1))\] <p>This leaves us with two terms to deal with here; the mean and the variance.</p> <p>For the mean, we can do this by either; (i) directly parameterising $x^{(0)}$ at each step, or (ii) by parameterising the noise directly $\epsilon^{(t)}$.</p> <h3 id="i-x0-parameterisation">i. $x^{(0)}$-parameterisation</h3> <table> <tbody> <tr> <td>This method involves looking at the noising step $q(x^{(t-1)}</td> <td>x^{(0)}, x^{(t)})$ where from Bayes rule, we have:</td> </tr> </tbody> </table> \[q(x^{(t-1)}|x^{(0)}, x^{(t)}) \propto q(x^{(t-1)}|x^{(0)})q(x^{(t)}|x^{(t-1)}) \\ = \mathcal{N}\left( x^{(t-1)};\ \left(\prod_{t^\prime = 1}^{t-1} \lambda_{t^\prime} x^{(0)} \right),\ 1 - \prod_{t^\prime = 1}^{t-1} \lambda_{t^{\prime}}^{2} \right) \times \mathcal{N}(x^{(t)};\ \lambda_t x^{(t-1)}, \ 1-\lambda_t^2)\] <table> <tbody> <tr> <td>then completing the square and doing some mathematical gymnastics gives us a new distribution $q(x^{(t-1)}</td> <td>x^{(0)}, x^{(t)}) = \mathcal{N}(x^{(t-1)}; \mu_{t-1</td> <td>0,t}, \sigma_{t-1</td> <td>0,t}^{2})$ where:</td> </tr> </tbody> </table> \[\mu_{t-1|0,t} = \frac{\left(\prod_{t^\prime = 1}^{t-1} \lambda_{t^\prime}\right)(1-\lambda_t^2)}{1 - \prod_{t^\prime = 1}^{t} \lambda^2_{t^\prime}} x^{(0)} + \frac{\left(1 - \prod_{t^\prime = 1}^{t-1} \lambda^2_{t^\prime}\right)\lambda_t}{1 - \prod_{t^\prime = 1}^{t} \lambda^2_{t^\prime}}x^{(t)} \\ \sigma_{t-1|0,t}^{2} = \frac{\left(1 - \prod_{t^\prime = 1}^{t-1} \lambda^2_{t^\prime}\right)(1 - \lambda_t^2)}{1 - \prod_{t^\prime = 1}^{t} \lambda^2_{t^\prime}}\] <p>In the $x^{(0)}$-parameterisation, we do exactly what it says on the tin and parameterise $x^{(0)}$:</p> \[\mu_{t-1|0,t}^{\theta} = a^{(t)}x^{(0)}_{\theta}(x^{(t)}) + b^{(t)}x^{(t)}\] <h3 id="ii-epsilon-parameterisation">ii. $\epsilon$-parameterisation</h3> <p>An alternative parameterisation to the above is found by instead aiming to predict the noise $\epsilon$ at each step. Similarly to the $x^{(0)}$-parameterisation, our weight sharing is well-founded by the argument that each step is adding a similar amount of noise - so we can imagine some redundancy in the parameters across adjacent steps.</p> <p>If we cast our minds back to earlier, we had a noising schedule which can be written as:</p> \[x^{(t)} = \prod_{t^\prime = 1}^{t} \lambda_{t^\prime} x^{(0)} + \sqrt{1-\prod_{t^\prime = 1}^{t}\lambda_{t^\prime}^{2}}\ \epsilon^{(t)} \qquad \epsilon^{(t)} \sim \mathcal{N}(0,1) \\ x^{(t)} = c^{(t)} x^{(0)} + d^{(t)} \epsilon^{(t)}\] <p>So if we take the conditional mean that we got from our $x^{(0)}$-parameterisation, we can easily substitue the above into this and write down a new expression of the form:</p> \[\mu_{t-1|0,t}^{\theta} = \left(\frac{a^{(t)}}{c^{(t)}} + b^{(t)}\right)x^{(t)} - \frac{a^{(t)}d^{(t)}}{c^{(t)}}\epsilon^{(t)}_{\theta}(x^{(t)})\] <p>So given the input and the timestep, our parameterisation now predicts the noise that needs to be taken away from $x^{(t)}$ in order to give $x^{(t-1)}$.</p> <p>OK, so hopefully its pretty easy to see how these two parameterisations are related to each other. In fact, you might be asking, <em>“what is the difference?”</em> or <em>“why would we want to do this?”</em> - these would be valid questions!</p> <p>To answer these a bit more in depth, we can consider how each one is actually performing the denoising step and at the same time, learn about some related work!</p> <h2 id="denoising-autoencoders-dae">Denoising Autoencoders (DAE)</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/diffusion/mnist_ae-480.webp 480w,/assets/img/blogs/diffusion/mnist_ae-800.webp 800w,/assets/img/blogs/diffusion/mnist_ae-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/diffusion/mnist_ae.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Autoencoder results from training to reconstruct MNIST digits. </div> <p>As discussed above, in the $x^{(0)}$-parameterisation, the goal is to predict the clean data $x^{(0)}$ from a noisy version $x^{(t)}$. The DAE is trained to map the noisy input $x^{(t)}$ to the clean output $x^{(0)}$. This process can be formulated as: \(x_{\theta}^{(0)} = f_{\theta}(x^{(t)})\)</p> <p>where $f_\theta$ is the neural network which parameterises $x_\theta^{(0)}$, $x^{(t)}$ is the noisy input at time step $t$, and $x^{(0)}$ is the predicted clean data.</p> <p>The training objective for the DAE in this context is to minimize the mean squared error (MSE) between the predicted clean data $x_{\theta}^{(0)}$ and the actual clean data $x^{(0)}$:</p> \[\mathcal{L}_{DAE} = \mathbb{E}_{x_0, t, \epsilon} \left[ | x^{(0)} - f_\theta(x^{(0)}+\epsilon) |^2 \right]\] <p>Here, $\epsilon$ represents the noise added to the clean data $x^{(0)}$ to obtain $x^{(t)}$. Taking a look at our objective for the $x^{(0)}$-parameterisation, we are aiming to achieve something which is $\propto x^{(0)} - x^{(0)}_{\theta}(x^{t}, t-1)$.</p> <p>Seeing it this way, we can see the resemblance of the denoising autoencoder to this specific parameterisation of a diffusion model.</p> <h2 id="denoising-score-matching-dsm">Denoising Score Matching (DSM)</h2> <p>Denoising score matching (DSM) is a technique used to estimate a score function. The score function is defined as <em>the gradient of the log probability density function of the data</em>. OK, slow down, what does that mean?</p> <table> <tbody> <tr> <td>Well, remember that $q(x^{(t-1)}</td> <td>x^{(0)}, x^{(t)})$ is our noising process. The score is the change in this noising process across the noising schedule, which changes with $x$. In the context of diffusion models, DSM is therefore closely related to the $\epsilon$-parameterisation, which essentially parameterises the change between steps in the noising process.</td> </tr> </tbody> </table> <p>In the $\epsilon$-parameterisation, the objective is to predict the noise added to the clean data $x^{(0)}$ to obtain the noisy data $x^{(t)}$. A neural network $g_\theta$ is trained to map the noisy input $x^{(t)}$ to the noise $\epsilon:</p> \[ϵ_{\theta} = g_{\theta}(x^{(t)})\] <p>As with our DAE discussion, lets have a look at a simple DSM training objective:</p> \[\mathcal{L}_{DSM} = \mathbb{E}_{x_0, t, \epsilon} \left[ | \epsilon - g_\theta(x^{(t)}) |^2 \right]\] <p>By minimizing this loss, the neural network learns to accurately predict the noise, which is essential for the $\epsilon$-parameterisation of diffusion models.</p> <p>Now we mentioned a score function, which is written as $\nabla_{x^{(t)}}\log p(x^{(t)})$ indirectly. Since the noise $\epsilon$ is related to the score function through the gradient of the log probability density, predicting $\epsilon$ effectively helps in estimating the score function.</p> <p>Anyway, score matching is an interesting topic which I’m not going to dwell too much on here but essentially it shares many concepts with denoising diffusion models - where the $\epsilon$-parameterisation is basically practical implementation of score matching.</p> <h2 id="defining-a-loss-function">Defining a Loss Function</h2> <p>OK, we’ve been around the block a bit and discussed some technical stuff, but just to wrap up this post, I’m going to drop a loss function here which was introduced by Ho, et. al., (2020) in the seminal paper on denoising diffusion models:</p> \[\mathcal{L}(\theta) = -\frac{T}{2}\mathbb{E}_{t}\left[\left(\epsilon^{(t)} - \epsilon^{(t)}_{\theta}(\Lambda_t x^{(0)} + \sqrt{1-\Lambda^{2}_t} \epsilon^{(t)}, t-1)\right)^2\right]\] <p>where $\Lambda_t = \prod_{t^\prime=1}^{t}\lambda_{t^\prime}$. Hopefully, you are able to see that this loss function follows quite nicely from some of the discussions we have had in earlier sections.</p> <p>OK, great! Thanks for reading this post, if you have made it this far! Although technical, I hope it has given you a bit more of an intuitive explanation behind some of the main mathematical concepts of diffusion models.</p>]]></content><author><name></name></author><category term="deep"/><category term="learning"/><category term="math"/><category term="diffusion"/><summary type="html"><![CDATA[A comprehensive look at the theory and concepts underpinning diffusion models.]]></summary></entry></feed>