<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jamie-mcg.github,io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jamie-mcg.github,io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-20T18:44:29+00:00</updated><id>https://jamie-mcg.github,io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">MLPs Are All You Need… For FLOP calculations</title><link href="https://jamie-mcg.github,io/blog/2025/flops/" rel="alternate" type="text/html" title="MLPs Are All You Need… For FLOP calculations"/><published>2025-02-20T15:12:00+00:00</published><updated>2025-02-20T15:12:00+00:00</updated><id>https://jamie-mcg.github,io/blog/2025/flops</id><content type="html" xml:base="https://jamie-mcg.github,io/blog/2025/flops/"><![CDATA[<p>OK, clearly this section heading is misleading. But let me explain! So, transformer blocks are essentially made up of an attention part and a feedforward part. This feedforward block is further composed of three dense layers as below:</p> <p>In maths, for a model with \(d_{\text{model}}\) embedded/model dimension and \(d_{\text{hidden}}\) hidden dimension, and an input with batch size \(N\) and sequence length \(T\), this can be written as:</p> \[X_{\text{out,ff}} = \sigma(X_{\text{in}} W_2 \odot X_{\text{in}} W_1) W_3\] <p>where \(X_{\text{in}} \in \R^{N\times T \times d_{\text{model}}}\), \(W_{1,2} \in \R^{d_{\text{model}} \times d_{\text{hidden}}}\) and \(W_{3} \in \R^{d_{\text{hidden}} \times d_{\text{model}}}\).</p> <p>Now, lets work out the computational burden this function has for a forward pass!</p> <ul> <li>There are two operations that require \(2NTd_{\text{model}}d_{\text{hidden}}\) FLOPs and memory \(d_{\text{model}}d_{\text{hidden}}\)</li> <li>An element-wise multiplication which is of the order \(NTd_{\text{hidden}}\) and no memory overhead</li> <li>A final operation which requires a further \(2NTd_{\text{model}}d_{\text{hidden}}\) and memory \(d_{\text{model}}d_{\text{hidden}}\)</li> </ul> <p>So in total we have \(6NTd_{\text{model}}d_{\text{hidden}}\) FLOPs.</p> <p>By contrast, for the Attention part, we have the following formulation:</p> \[A^h = \text{softmax} \left(\frac{X_{\text{in}}W^h_q \cdot {W_k^{h}}^T X_{\text{in}}^T}{\sqrt{d_h}}\right) \\ Y^h = A^h \cdot X_{\text{in}}W^h_v \\ X_{\text{out}} = Y^{h} W_{o}\] <p>where \(d_{\text{h}}\) is the attention head dimension, \(W_{q,k,v}^h \in \R^{d_{\text{model}} \times d_{\text{h}}}\), \(h\) is the attention head index which runs up to the total number of attention heads \(H\) and \(W_{o} \in \R^{H d_h \times d_{\text{model}}}\) .</p> <ul> <li>There are three multiplications in here, each requiring \(2NTd_{\text{model}}d_{\text{h}}H\) and \(Hd_{\text{model}}d_{\text{h}}\) memory.</li> <li>There is a dot product in the softmax operation which requires \(2NT^2d_{h}H\) FLOPs (where we have a squared sequence length because we’re creating the lookup table over all tokens in the sequence).</li> <li>Similarly, we have another dot product between the attention matrix and the value which involves \(2NT^2d_{h}H\).</li> <li>Finally, we have the last matmul operation which multiplies and reduces all the attention head outputs into our final output, which requires \(2NTd_{\text{model}}d_{\text{h}}H\) and \(Hd_{\text{model}}d_{\text{h}}\) memory.</li> </ul> <p>In total, that leaves us with \(8NTd_{\text{model}}d_{\text{h}}H + 4NT^2d_{h}H\) FLOPs, where the first term is from the MLP block and the second is all down to attention.</p> <p>Just to analyse this a bit more, lets investigate the relative difference between these two terms. Factoring common terms out we get:</p> \[4NTd_{h}H(2d_{\text{model}} + T) \simeq 8NTd_{\text{model}}d_{\text{h}}H \qquad \text{when} \quad d_{\text{model}} \gg T/2\] <p>where the MLPs dominate the FLOP count whenever \(d_{\text{model}}\) is higher than the context size.</p> <p>Right, so now lets just put this into context. Let’s consider LLaMA 3-70B, which has \(d_{model} = 8192\) and therefore we can get a pretty good approximation to the compute costs of this model whenever our sequence length is less than ~4k tokens!</p> <h3 id="a-quick-note-on-training">A quick note on training</h3> <p>The above was all done for inference, of course, for training things get slightly more complicated in terms of memory and FLOPs - especially when you start considering different checkpointing strategies for intermediate activations, different optimizer states, and other parallelisation techniques. However, we can make a simple adaptation to the FLOP calculations we did above but just considering the chain rule and backpropagation.</p> <p>So, in training, gradients are essential for us to compute. Now imagine a set of feedforward layers stacked on top of each other, the computational graph produced from a 2-layer network looks like Figure 2.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/FLOPs/backprop-graph-480.webp 480w,/assets/img/blogs/FLOPs/backprop-graph-800.webp 800w,/assets/img/blogs/FLOPs/backprop-graph-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/FLOPs/backprop-graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Computational graph from torchviz of a 2-layer MLP with annotations to show the leaf node computations (red) and backpropgated vectors for use in previous layers through vector-Jacobian products (green). Image created by author. </div> <p>The goal of backpropagation is to compute a gradient with respect to the current layer weights (this is what’s called a “leaf” in your computational graph), as well as the gradient with respect to the input of that layer.</p> <p>Why do we need these? Well, the former we’re going to use in our update equation to update the weights of that layer (red path in Figure 3), and the latter we are going to pass down to the previous layer as our new vector in the vector-Jacobian-product chain (green path in Figure 3).</p> <p>Don’t worry too much about the details of this here! All you need to really know is that there are 2 extra computations in the backwards pass, so you basically just want to add a factor of 3 to all the FLOP calculations we did above!</p>]]></content><author><name></name></author><category term="engineering"/><category term="LLM"/><category term="LLM"/><category term="transformer"/><summary type="html"><![CDATA[A run down of the major operations you need to consider when calculating the computational burden from your transformer]]></summary></entry><entry><title type="html">Convexity Explained</title><link href="https://jamie-mcg.github,io/blog/2024/convexity/" rel="alternate" type="text/html" title="Convexity Explained"/><published>2024-02-11T15:12:00+00:00</published><updated>2024-02-11T15:12:00+00:00</updated><id>https://jamie-mcg.github,io/blog/2024/convexity</id><content type="html" xml:base="https://jamie-mcg.github,io/blog/2024/convexity/"><![CDATA[<h2 id="what-is-convexity">What is convexity?</h2> <blockquote> <p>A function is convex if the line segment between two points lies above the function.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/convexity/jensens-480.webp 480w,/assets/img/blogs/convexity/jensens-800.webp 800w,/assets/img/blogs/convexity/jensens-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/convexity/jensens.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A visual representation of a convex function with an intersection forming an enclosed area above the curve. Figure created by author. </div> <p>In the above figure, we see an illustrative example of the definition above. Here, we can see that if the intersection of the line forms an enclosed area which is uninterrupted above the curve, the function can be considered convex.</p> <p>We can also see that if the opposite is true i.e. the enclosed area is below the curve, the function is concave!</p> <p>Indeed, a convex function \(f(x)\) can be reflected into a concave function \(-f(x)\).</p> <p>Mathematically, convexity is best described by Jensen’s inequality,</p> <p>\(f(E[X]) \le E[f(X)]\) for a convex function \(f\) and a random vector \(X\) that is within the domain of \(f\). Now, I find it hard to conceptualise inequalities at face value, so let’s chat about it a bit…</p> <p>Jensen’s inequality essentially states that the expected value of a convex transformation of a random variable (i.e. \(E[f(X)]\)) is more than or equal to the value of the convex function evaluated at the mean od the random variable (i.e. \(f(E[X])\)) - or the gap between these two is never negative.</p> <p>OK, I’ll be honest, when I wrote that, it only half made sense to me… I’m a physicist by background and so I love drawings and building intuitions visually, so let’s do that!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/convexity/jensens_inequality-480.webp 480w,/assets/img/blogs/convexity/jensens_inequality-800.webp 800w,/assets/img/blogs/convexity/jensens_inequality-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/convexity/jensens_inequality.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An animated representation of Jensen's inequality. Figure created by author. </div> <p>Consider the figure above, here it is a lot easier to see what Jensen’s inequality is really saying. Basically the LHS of Jensen’s inequality is represented by the green dotted line - this is taking the average of the two red \(x\) points and then evaluating the function at this point (i.e. \(f(E[X])\)). Whereas, the intersection of the red dashed line and the green dotted line is what the RHS of Jensen’s inequality is describing - evaluating the function before taking the expectation (i.e. \(E[f(X)]\)).</p> <p>We can see that over the entire range of this function, the green dot never crosses the red line and so the intersection of the green dotted and red dashed lines is always above the blue solid curve.</p> <p>OK, hopefully that was a nice and easy to follow explanation for you! And if it wasn’t, at least you got to distract yourself with a cool animation! Anyway, let’s get back to convexity…</p> <p>OK, so as well as Jensen’s inequality, for a twice differentiable function which is convex, we have two rather nice conditions:</p> \[f^{''}(x) &gt;= 0 \qquad \forall x, \\ f(y) &gt;= f(x) + \nabla f(x)^{T} (y-x).\] <p>The first of these conditions tells us that curvature must be positive everywhere. The second condition ensures that the line between two points \(x\) and \(y\) lies above the function within that interval (illustrated earlier).</p> <p>Following on from this, the first condition ensures that a convex function must curve upwards (or not at all). Therefore, if we find a minima x, any movement away from this point will result in an increase in the function value.</p> <p>Together, these conditions are enough to guarantee that any minima found in a convex function must be a global minima (although this doesn’t necessarily need to be unique). This is extremely useful for techniques such as stochastic Gradient Descent since if we find a local minima (something which Gradient Descent generally tends to find), we can be certain that this in fact the global minima - the best possible solution.</p> <h2 id="the-strong-the-strict-and-the-standard">The Strong, the Strict and the Standard</h2> <p>As always with math, we don’t just have one type of something, we have some vague terms that sound cool to describe some more features.</p> <p>Convex functions are no exception and can be further categorised with three properties:</p> <ul> <li>Convex</li> <li>Strictly Convex</li> <li>Strongly Convex</li> </ul> <p>Now, the further you move down that list, the stronger these properties become (hence the “strongly” term). This just means that the subset of convex functions that have these properties becomes smaller due to stronger constraints.</p> <p>So far in this post, we have been talking about convexity in its most general form, where \(f^{''}(x) &gt;= 0\). This is the condition for ‘convex’ functions.</p> <p>Strictly convex functions are those which satisfy \(f^{''}(x) &gt; 0\), i.e. the curvature can never be 0.</p> <p>Stongly convex functions are those which satisfy \(f^{''}(x) &gt;= m &gt; 0\), where the curvature is non-vanishing and stays bounded below by some positive value \(m\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/convexity/strong_and_strict-480.webp 480w,/assets/img/blogs/convexity/strong_and_strict-800.webp 800w,/assets/img/blogs/convexity/strong_and_strict-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/convexity/strong_and_strict.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A plot showing examples of different types of convex functions. Figure created by author. </div> <p>In the above plot, we can see the differences between these types of convexities visually. If you take the definitions in the legend and differentiate these twice then it is easy to prove to yourself that each of these are convex, strictly convex or strongly convex.</p> <p>OK cool, well done for reading another one of my posts! I’m trying to keep these as short and sweet as possible and just give the “need-to-know” understanding for these concepts. Let me know how I did!</p>]]></content><author><name></name></author><category term="mathematics"/><category term="math"/><category term="optimisation"/><summary type="html"><![CDATA[A short and easy to follow primer on convexity.]]></summary></entry><entry><title type="html">What the Lipschitz?!</title><link href="https://jamie-mcg.github,io/blog/2024/lipschitz/" rel="alternate" type="text/html" title="What the Lipschitz?!"/><published>2024-01-29T15:12:00+00:00</published><updated>2024-01-29T15:12:00+00:00</updated><id>https://jamie-mcg.github,io/blog/2024/lipschitz</id><content type="html" xml:base="https://jamie-mcg.github,io/blog/2024/lipschitz/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Lipschitzness of a function is essential for ensuring the convergence properties of many gradient descent algorithms</p> <h2 id="definition">Definition</h2> <p>Simply put, Lipschitz functions are those which do not explode for some value \(x\). So, functions which change too fast and/or become infinitely steep are not Lipschitz functions.</p> <p>More formally, let \(\chi \in \mathbb{R}^{d}\) be a d-dimensional subspace of real values. If we take a function \(f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{n}\) which provides a mapping from a d-dimensional space to a p-dimensional space, we can say that \(f\) is \(L\)-Lipschitz over \(\chi\) if and only if we have:</p> \[|f(x_{2}) - f(x_{1})| \le L | x_{1} - x_{2} | \qquad \forall x_{1}, x_{2} \in \chi\] <p>I always think this looks a bit confusing written this way, so a more intuitive way to write it is simply:</p> \[\frac{|f(x_{2}) - f(x_{1})|}{ | x_{1} - x_{2} | } \le L \qquad \forall x_{1}, x_{2} \in \chi\] <p>where we now have the form of \(\Delta y / \Delta x\) on the left hand side.</p> <p>Thinking about this a bit more, this condition demands that the slope of the secant line between two points \(x_{1}\) and \(x_{2}\) must be between \(-L \le m \le L\).</p> <h3 id="example-is-cosine-lipschitz">Example: Is cosine Lipschitz?</h3> <p>Start by employing the definition above,</p> \[\frac{f(x) - f(y)}{x - y} \simeq f^{'}(x) = \text{sin}(x)\] <table> <tbody> <tr> <td>We know that $$</td> <td>\text{sin}(x)</td> <td>\le 1$$, so we can rewrite this as:</td> </tr> </tbody> </table> \[|f(x) - f(y)| \le L | x - y |\] <p>where \(L = 1\). So we say that cosine is a \(1\)-Lipschitz function.</p> <p>Now, if you’re screaming “Stop showing me maths!!”, you’re in luck, because I’ve created some nice plots for us to look at…</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/lipschitz/lipschitz_curves-480.webp 480w,/assets/img/blogs/lipschitz/lipschitz_curves-800.webp 800w,/assets/img/blogs/lipschitz/lipschitz_curves-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/lipschitz/lipschitz_curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Plots showing linear, cosine, cusp and quadratic functions to showcase Lipschitzness. Image created by author. </div> <p>So, remembering our rule from above, we can see that the linear and cosine functions both satisfy the Lipschitz inquality as there is some bound on the slopes they exhibit for all \(x \in \chi\). On the other hand, the cusp function is not Lipschitz continuous at the origin because it has a discontinuity. Finally, the quadratic function is defined as Lipschitz continuous if and only if we are considering a bounded interval, because as \(x \rightarrow \infty\), the slope becomes arbitrarily large.</p> <h2 id="globally--locally-lipschitz">Globally &amp; Locally Lipschitz</h2> <p>Just to round off this small post, I want to talk a bit about global and local Lipschitz functions. The above section kind of describes functions which are globally Lipschitz since we haven’t defined a subset of the function space to consider, so here, we’ll start local!</p> <p>Say for example, we have some function \(f\) which is locally Lipschitz for a compact subset of \(\chi\). We’ll call this \(\Omega \subset \chi \in \mathbb{R}^{d}\).</p> <p>For the local Lipschitz property to hold, it must be true that there is a constant \(L_{\Omega}\) such that,</p> \[|f(x) - f(y)| \le L_{\Omega} | x - y | \qquad \forall x_{1}, x_{2} \in \Omega\] <p>where \(L_{\Omega}\) can indeed depend on the subset \(\Omega\). For example, the function \(f(x) = x^{2}\) has an \(L_{\Omega}\) which depends on \(x\) (since only one of the \(x^{2}\) will cancel). Therefore as the subset \(\Omega\) becomes larger, \(L_{\Omega}\) will scale linearly with this.</p> <p>The above example defines a situation where we have local Lipschitzness but not global!</p> <p>For global Lipschitzness, we require the function to have a Lipschitz constant which does not depend on the subset \(\Omega\), i.e. \(L_{\Omega} = L\).</p> <p>Now, just looking back at our plot from earlier, we can observe that the linear and cosine functions are most definitely locally Lipschitz because they are globally Lipschitz continuous. But although the cusp and quadratic functions were not globally Lipschitz, they can indeed be locally Lipschitz. For example, the problem with the cusp function was just the point at the origin. So if we just ignore this point (define our compact subspace accordingly), we can find a Lipschitz constant. Then, for our quadratic function, we said that it needed to be a bounded interval before we could call it Lipschitz continuous. But the definition of local Lipschitzness is that the “local” is on a compact subspace which, you guessed it, is bounded! Thus a Lipschitz constant exists for any compact subset on the quadratics domain.</p> <p>I hope this post has been a useful primer into the property of Lipschitzness. As always, feel free to reach out with any comments/questions!</p>]]></content><author><name></name></author><category term="mathematics"/><category term="math"/><summary type="html"><![CDATA[All the math and intuition you need to know for a happy life.]]></summary></entry></feed>